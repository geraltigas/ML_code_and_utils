{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "sys.path.append('../.')\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch import nn, optim\n",
    "from torch.functional import F\n",
    "from myutils.template import train_model\n",
    "from myutils.python_and_os import ignore_warnings\n",
    "\n",
    "ignore_warnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'project': 'LSTM',\n",
    "    'lr': 1e-3,\n",
    "    'epoch': 30,\n",
    "    'batch_size':1,\n",
    "    'dataloader_shuffle': True,\n",
    "    'momentum': 0.9,\n",
    "    'device': 'cuda',\n",
    "    'save': True,\n",
    "    'save_dir': \"epoch_{}_{:.3f}.pth\",\n",
    "    'LSTM_window':20\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dem:int,hs_dem:int):\n",
    "\n",
    "        super(MyLSTM,self).__init__()\n",
    "\n",
    "        self.hs_dem = hs_dem\n",
    "\n",
    "        # none grad parameter\n",
    "        self.cell_state = torch.zeros(hs_dem)\n",
    "        self.hidden_state = torch.zeros(hs_dem)\n",
    "\n",
    "        # require grad parameter\n",
    "        self.w_xi = nn.Parameter(torch.randn(input_dem,hs_dem),requires_grad=True)\n",
    "        self.w_xf = nn.Parameter(torch.randn(input_dem,hs_dem),requires_grad=True)\n",
    "        self.w_xo = nn.Parameter(torch.randn(input_dem,hs_dem),requires_grad=True)\n",
    "        self.w_xc = nn.Parameter(torch.randn(input_dem,hs_dem),requires_grad=True)\n",
    "\n",
    "        self.w_hi = nn.Parameter(torch.randn(hs_dem,hs_dem),requires_grad=True)\n",
    "        self.w_hf = nn.Parameter(torch.randn(hs_dem,hs_dem),requires_grad=True)\n",
    "        self.w_ho = nn.Parameter(torch.randn(hs_dem,hs_dem),requires_grad=True)\n",
    "        self.w_hc = nn.Parameter(torch.randn(hs_dem,hs_dem),requires_grad=True)\n",
    "\n",
    "        self.b_i = nn.Parameter(torch.zeros(hs_dem),requires_grad=True)\n",
    "        self.b_f = nn.Parameter(torch.zeros(hs_dem),requires_grad=True)\n",
    "        self.b_o = nn.Parameter(torch.zeros(hs_dem),requires_grad=True)\n",
    "        self.b_c = nn.Parameter(torch.zeros(hs_dem),requires_grad=True)\n",
    "\n",
    "    def _forward(self,x):\n",
    "        i = F.sigmoid(torch.matmul(x,self.w_xi) + torch.matmul(self.hidden_state,self.w_hi) + self.b_i)\n",
    "        f = F.sigmoid(torch.matmul(x,self.w_xf) + torch.matmul(self.hidden_state,self.w_hf) + self.b_f)\n",
    "        o = F.sigmoid(torch.matmul(x,self.w_xo) + torch.matmul(self.hidden_state,self.w_ho) + self.b_o)\n",
    "        c = F.tanh(torch.matmul(x,self.w_xc) + torch.matmul(self.hidden_state,self.w_hc) + self.b_c)\n",
    "\n",
    "        self.cell_state = torch.mul(f,self.cell_state) + torch.dot(i,c)\n",
    "        self.hidden_state = torch.mul(o,torch.tanh(self.cell_state))\n",
    "\n",
    "        return self.hidden_state\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self._forward(x)\n",
    "\n",
    "    def clean_states(self):\n",
    "        self.cell_state = torch.zeros(self.hs_dem)\n",
    "        self.hidden_state = torch.zeros(self.hs_dem)\n",
    "\n",
    "class MySinPreModel(nn.Module):\n",
    "    def __init__(self,input_num:int,input_dem:int,hs_dem:int):\n",
    "        super(MySinPreModel,self).__init__()\n",
    "        self.input_num = input_num\n",
    "        self.lstm = MyLSTM(input_dem=input_dem,hs_dem=hs_dem)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hs_dem,1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        for index in range(self.input_num):\n",
    "            self.lstm(x[index])\n",
    "        return_value = self.mlp(self.lstm.hidden_state)\n",
    "        self.lstm.clean_states()\n",
    "        return return_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dataset define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MySinDataSet(Dataset):\n",
    "    def __init__(self,input_size:int,dataset_size:int,start:float = None,end:float = None):\n",
    "        import numpy as np\n",
    "        self.dataset_size = dataset_size\n",
    "        self.input_size = input_size\n",
    "        self.x = np.linspace(start=start,stop=end,num=dataset_size+input_size)\n",
    "        self.data = np.sin(self.x)\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "    def __getitem__(self, item):\n",
    "        return torch.asarray(self.data[item:item+self.input_size]),torch.asarray(self.data[item+self.input_size+1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DataLoader define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=MySinDataSet(config['LSTM_window'],100000,start=0,end=1000),batch_size=config['batch_size'],shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=MySinDataSet(config['LSTM_window'],10000,start=1001,end=1101),batch_size=config['batch_size'],shuffle=True)\n",
    "valid_dataloader = DataLoader(dataset=MySinDataSet(config['LSTM_window'],10000,start=1102,end=1202),batch_size=config['batch_size'],shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = MySinPreModel(config['LSTM_window'],1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Loss Func and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fine tune param select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# other_param = []\n",
    "# layer4_param = []\n",
    "# fc_param = []\n",
    "# i = 0\n",
    "#\n",
    "# for name, param in  model.named_parameters():\n",
    "#     if \"layer4\" in name:\n",
    "#         layer4_param.append(param)\n",
    "#     elif \"fc\" in name:\n",
    "#         fc_param.append(param)\n",
    "#     else:\n",
    "#         other_param.append(param)\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss and Optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "loss_func = loss_func.to(config['device']) # using cuda\n",
    "optimizer = optim.SGD(model.parameters(),config['lr'],momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mgeraltigas\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.2"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/geraltigas/ML/LSTM/wandb/run-20220826_212332-193b31em</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/geraltigas/LSTM/runs/193b31em\" target=\"_blank\">major-morning-4</a></strong> to <a href=\"https://wandb.ai/geraltigas/LSTM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/100000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, got 100, 100x1,20",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 27>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     26\u001B[0m wandb\u001B[38;5;241m.\u001B[39minit(project\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproject\u001B[39m\u001B[38;5;124m'\u001B[39m],config\u001B[38;5;241m=\u001B[39mconfig)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m'\u001B[39m]):\n\u001B[0;32m---> 28\u001B[0m     epoch_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdata_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mloss_func\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43mepoch_num\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43moutput_process\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_process\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ML/LSTM/.././myutils/template.py:21\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(config, model, data_loader, loss_func, optimizer, epoch_num, output_process)\u001B[0m\n\u001B[1;32m     18\u001B[0m featrues:Tensor \u001B[38;5;241m=\u001B[39m featrues\u001B[38;5;241m.\u001B[39mto(config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevice\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     19\u001B[0m labels:Tensor \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevice\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 21\u001B[0m pres \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatrues\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m pres \u001B[38;5;241m=\u001B[39m output_process(pres)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m     24\u001B[0m labels \u001B[38;5;241m=\u001B[39m output_process(labels)\u001B[38;5;241m.\u001B[39mfloat()\n",
      "Input \u001B[0;32mIn [3]\u001B[0m, in \u001B[0;36mMySinPreModel.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m,x):\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_num):\n\u001B[0;32m---> 62\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m     return_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm\u001B[38;5;241m.\u001B[39mhidden_state)\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlstm\u001B[38;5;241m.\u001B[39mclean_states()\n",
      "File \u001B[0;32m~/.conda/envs/ML/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[0;32mIn [3]\u001B[0m, in \u001B[0;36mMyLSTM.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m,x):\n\u001B[0;32m---> 41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [3]\u001B[0m, in \u001B[0;36mMyLSTM._forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_forward\u001B[39m(\u001B[38;5;28mself\u001B[39m,x):\n\u001B[0;32m---> 30\u001B[0m     i \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msigmoid(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mw_xi\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_state,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw_hi) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb_i)\n\u001B[1;32m     31\u001B[0m     f \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msigmoid(torch\u001B[38;5;241m.\u001B[39mmatmul(x,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw_xf) \u001B[38;5;241m+\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_state,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw_hf) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb_f)\n\u001B[1;32m     32\u001B[0m     o \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msigmoid(torch\u001B[38;5;241m.\u001B[39mmatmul(x,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw_xo) \u001B[38;5;241m+\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden_state,\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw_ho) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb_o)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: size mismatch, got 100, 100x1,20"
     ]
    }
   ],
   "source": [
    "# # test process define\n",
    "# def test_model(config:dict,model:nn.Module,test_dataLoader:DataLoader): #TODO: complete the test template code\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#\n",
    "#         num_total = 0\n",
    "#         acc_total = 0\n",
    "#         for i,(input,target) in enumerate(test_dataLoader):\n",
    "#             input = input.to(config['device'])\n",
    "#             output:torch.Tensor = model(input)\n",
    "#             out_index = F.softmax(output).argmax(dim=1)\n",
    "#             target = target.to(config['device'])\n",
    "#             if (out_index == target)[0].item():\n",
    "#                 acc_total += 1\n",
    "#             num_total += 1\n",
    "#\n",
    "#         wandb.log({\n",
    "#             'test_acc': acc_total/num_total\n",
    "#         })\n",
    "#         return acc_total/num_total\n",
    "\n",
    "# process after model output and before loss func\n",
    "def output_process(output:torch.Tensor):\n",
    "    return output\n",
    "\n",
    "wandb.init(project=config['project'],config=config)\n",
    "for epoch in range(config['epoch']):\n",
    "    epoch_loss = train_model(config=config,model=model,data_loader=train_dataloader,loss_func=loss_func,optimizer=optimizer,epoch_num=epoch,output_process=output_process)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}